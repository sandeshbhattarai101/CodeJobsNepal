{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c67e8630-271b-49b8-b22f-e23913a61162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "80943de7-663a-44f9-993a-f4096283ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "\n",
    "\n",
    "\n",
    "# Finding total pages in the website\n",
    "\n",
    "listing_url = 'https://merojob.com/category/it-telecommunication/?page=1'\n",
    "response = requests.get(listing_url, headers = headers)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "pagination = soup.find('ul', class_= 'pagination')\n",
    "\n",
    "if pagination:\n",
    "    # find the tatal page numbers except 'next' or 'previous'\n",
    "    pages = pagination.find_all('li')\n",
    "    page_numbers = []\n",
    "\n",
    "    for page in pages:\n",
    "        try:\n",
    "            num = int(page.text.strip())   \n",
    "            page_numbers.append(num)          \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    max_page = max(page_numbers) if page_numbers else 1\n",
    "else:\n",
    "    max_page = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1ea48d73-5d94-4e59-95fe-10aec41447d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_list = []\n",
    "final = pd.DataFrame()\n",
    "\n",
    "#including base url because href only includes page address\n",
    "base_url = \"https://merojob.com\"\n",
    "    \n",
    "# we are not using range(max_page+1) because it will start from 0 by default\n",
    "for i in range(1, (max_page + 1)):\n",
    "    webpage = requests.get('https://merojob.com/category/it-telecommunication/?page={}'.format(i), headers = headers).text\n",
    "    \n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "\n",
    "    # In website from where data is scraped, jobs are under classname \"job-card\"\n",
    "    #.text is only used on single items so using loop to seperate single item from list\n",
    "    #.strip() is used to remove any trailing whitespaces and namespaces\n",
    "    \n",
    "    job_cards = soup.find_all('h1', class_= 'media-heading')\n",
    "    specific_cards = soup.find_all('hw')\n",
    "\n",
    "    # if not job_cards:\n",
    "    #     print('No jobs found on this page, stopping.')\n",
    "    #     break\n",
    "\n",
    "    for job in job_cards:\n",
    "        link_tag = job.find('a', href= True)\n",
    "        if link_tag:\n",
    "            job_url = base_url + link_tag['href']\n",
    "            # print(f\"Visiting: {job_url}\")\n",
    "\n",
    "            job_detail_resp = requests.get(job_url, headers = headers)\n",
    "            job_soup = BeautifulSoup(job_detail_resp.text, 'lxml')\n",
    "        \n",
    "            \n",
    "            if job_soup:\n",
    "\n",
    "                job_data = {}\n",
    "                \n",
    "                \n",
    "                head_info = job_soup.find('div', class_= 'media-body mt-3')\n",
    "                \n",
    "                job_title_info = head_info.find_all('h1')\n",
    "\n",
    "                #there are more than one element in head_info so    \n",
    "                for title in job_title_info:\n",
    "                    job_data['Job Title'] = title.text.strip()\n",
    "\n",
    "                company_name_info = head_info.find_all('a')\n",
    "                \n",
    "                for name in company_name_info:\n",
    "                    job_data['Company Name'] = name.text.strip()\n",
    "\n",
    "            \n",
    "\n",
    "                table_info = job_soup.find_all('table', class_= 'table')\n",
    "\n",
    "                for table in table_info:\n",
    "                    for row in table.find_all('tr'):\n",
    "                         cells = row.find_all(['th', 'td'])\n",
    "                         if cells:\n",
    "                             key = cells[0].text.strip()\n",
    "                             value = cells[2].text.strip().replace('\\xa0', ' ').replace('\\n', ' ').strip()\n",
    "                             job_data[key] = value\n",
    "                \n",
    "                # print(job_data)             \n",
    "                job_list.append(job_data.copy())\n",
    "                # print(job_list)\n",
    "\n",
    "# print(f\"Total number of jobs:{len(job_list)}\")\n",
    "df = pd.DataFrame(job_list)\n",
    "df.to_csv('merojob_jobs.csv')\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e27e70-9895-47c7-b43c-84b96b9497b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
